using Microsoft.ML;
using Microsoft.ML.OnnxRuntime;
using Microsoft.ML.OnnxRuntime.Tensors;
using System;
using System.Collections.Generic;
using System.Linq;
using System.Threading;

namespace TestOnnxInferenceSpeed
{
    class Program
    {
        private static readonly string ModelPath = "./onnx_inference_model.onnx";
        static void Main(string[] args)
        {
            Console.WriteLine("Hello World!");

            var rand = new Random();
            Console.WriteLine("Generationg random data");
            var inputDimensions = new int[] { 250, 15, 15, 2 };
            //var inputDataList = new List<InputData>();
            //for(int i = 0; i < 1000; i++)
            //{
            //    var inputData = new InputData()
            //    {
            //        Input = new float[450]
            //    };

            //    for(int j = 0; j < inputData.Input.Length; j++)
            //    {
            //        inputData.Input[j] = rand.Next(2);
            //    }
            //    inputDataList.Add(inputData);
            //}
            var inputData = new float[inputDimensions[0] * inputDimensions[1] * inputDimensions[2] * inputDimensions[3]];
            for (int i = 0; i < inputData.Length; i++)
            {
                inputData[i] = (float)rand.NextDouble();
            }
            var threads = new List<Thread>();

            Console.WriteLine("Prepearing threads");
            for (int t = 0; t < 8; t++)
            {
                threads.Add(new Thread(() =>
                {
                    
                    int? gpu = null;
                    //var mlContext = new MLContext();
                    //var dummyData = mlContext.Data.LoadFromEnumerable(new InputData[0]);
                    //var pipeline = mlContext.Transforms.ApplyOnnxModel(ModelPath, gpuDeviceId: gpu);
                    //var transformer = pipeline.Fit(dummyData);
                    //var predictionEngine = mlContext.Model.CreatePredictionEngine<InputData, Prediction>(transformer);
                    var inferenceSession = new InferenceSession(ModelPath, SessionOptions.MakeSessionOptionWithCudaProvider(0));
                    

                    for (int i = 0; i < 1000; i++)
                    {
                        
                        var inputs = new List<NamedOnnxValue>()
                        {
                            NamedOnnxValue.CreateFromTensor("input", new DenseTensor<float>(inputData, inputDimensions))
                        };
                        var outputs = new List<string>()
                        {
                            "output"
                        };
                        //inputs.AddRange(
                        //    inputDataList.Select(x => NamedOnnxValue.CreateFromTensor("input",new DenseTensor<float>(x.Input, inputDimensions)))
                        //    );

                        using (var results = inferenceSession.Run(inputs, outputs))
                        {
                            // manipulate the results
                            var result = results.First();
                            //foreach(var item in result.AsEnumerable<float>())
                            //{
                            //    Console.WriteLine(item);
                            //}
                            var count = result.AsEnumerable<float>().Count();
                        }

                        //var data = mlContext.Data.LoadFromEnumerable(inputDataList);
                        //foreach (var item in inputDataList)
                        //{
                        //    _ = predictionEngine.Predict(item).Evaluation;
                        //}
                        //var transformedDataView = transformer.Transform(data);
                        //var transformedData = mlContext.Data.CreateEnumerable<Prediction>(
                        //    transformedDataView,
                        //    reuseRowObject: true
                        //).Select(x => x.Evaluation);
                        //Console.WriteLine(transformedData.Count());
                    }
                }));
            }
            Console.WriteLine("Starting inference");
            var start = DateTime.Now;
            foreach (var thread in threads)
                thread.Start();

            foreach (var thread in threads)
                thread.Join();

            var duration = DateTime.Now - start;
            Console.WriteLine($"###############DONE in {duration.TotalSeconds} seconds################");

        }
    }
}
